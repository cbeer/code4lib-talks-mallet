[{"title":"VuFind 2.0: Why and How? ","id":341,"description":"Demian Katz, Villanova University, demian.katz@villanova.edu \r\n\r\nA major new version of the VuFind discovery software is currently in development. While VuFind 1.x remains extremely popular, some of its components are beginning to show their age. VuFind 2.0 aims to retain all the strengths of the previous version of the software while making the architecture cleaner, more modern and more standards-based. This presentation will examine the motivation behind the update, preview some of the new features to look forward to, and discuss the challenges of creating a developer-friendly open source package in PHP.","score":135},{"title":"Open Source Software Registry","id":342,"description":"Peter Murray, LYRASIS, Peter.Murray@lyrasis.org \r\n\r\nLYRASIS is creating and shepherding a [registry of library open source software](http://wiki.code4lib.org/index.php/Registry_E-R_Diagram) as part of its [grant from the Mellon Foundation to support the adoption of open source software by libraries](http://www.lyrasis.org/News/Press-Releases/2011/LYRASIS-Receives-Grant-to-Support-Open-Source.aspx).  \r\nThe goal of the grant is to help libraries of all types determine if open source software is right for them, and what combination of software, hosting, training, and consulting works for their situation.  \r\nThe registry is intended to become a community exchange point and stimulant for growth of the library open source ecosystem by connecting libraries with projects, service providers, and events.\r\n\r\nThe first half of this session will demonstrate the registry functions and describe how projects and providers can get involved.  \r\nThe second half of the session will be a brainstorming suggestion of how to expand the functionality and usefulness of the registry.\r\n","score":100},{"title":"Property Graphs And TinkerPop Applications in Digital Libraries ","id":343,"description":"Brian Tingle, California Digital Library, brian.tingle.cdlib.org@gmail.com \r\n\r\n[TinkerPop](http://www.tinkerpop.com/) is an open source software development group focusing on technologies in the [graph database](http://en.wikipedia.org/wiki/Graph_database) space.   \r\nThis talk will provide a general introduction to the TinkerPop Graph Stack and the [property graph model](https://github.com/tinkerpop/gremlin/wiki/Defining-a-Property-Graph) is uses.  The introduction will include code examples and explanations of the property graph models used by the [Social Networks in Archival Context](http://socialarchive.iath.virginia.edu/) project and show how the historical social graph is exposed as a JSON/REST API implemented by a TinkerPop [rexster](https://github.com/tinkerpop/rexster) [Kibble](https://github.com/tinkerpop/rexster-kibbles) that contains the application's graph theory logic.  Other graph database applications possible with TinkerPop such as RDF support, and citation analysis will also be discussed.","score":104},{"title":"Security in Mind ","id":344,"description":"Erin Germ, United States Naval Academy, Nimitz Library, germ@usna.edu \r\n\r\nI would like to talk about security of library software.\r\n\r\nOver the Summer, I discovered a critical vulnerability in a vendor\u2019s software that (verified) allowed me to assume any user\u2019s identity for that site, (verified) switch to any user, and to (unverified, meaning I didn\u2019t not perform this as I didn\u2019t want to \u201chack\u201d another library\u2019s site) assume the role of any user for any other library who used this particular vendor's software.\r\n\r\nWithin a 3 hour period, I discovered a 2 vulnerabilities: 1) minor one allowing me to access any backups from any library site, and 2) a critical vulnerability. From start to finish, the examination, discovery in the vulnerability, and execution of a working exploit was done in less than 2 hours. The vulnerability was a result of poor cookie implementation. The exploit itself revolved around modifying the cookie, and then altering the browser\u2019s permissions by assuming the role of another user.\r\n\r\nI do not intend on stating which vendor it was, but I will show how I was able to perform this. If needed, I can do further research and \u201cinvestigation\u201d into other vendor's software to see what I can \u201cfind\u201d. \r\n\r\n*If selected, I will contact the vendor to inform them that I will present about this at C4L2012. I do not intend on releasing the name of the vendor.*","score":137},{"title":"Search Engines and Libraries ","id":345,"description":"Greg Lindahl, blekko CTO, greg@blekko.com \r\n\r\n[blekko](https://blekko.com) is a new web-scale search engine which enables end-users to create vertical search engines, through a feature called [slashtags](http://help.blekko.com/index.php/category/slashtags/). Slashtags can contain as few as 1 or as many as tens of thousands of websites relevant to a narrow or broad topic. We have an extensive set of slashtags curated by a combination of volunteers and an in-house librarian team, or end-users can create and share their own. This talk will cover examples of slashtag creation relevant to libraries, and show how to embed this search into a library website, either using javascript or via our API.\r\n\r\n*We have exhibited at a couple of library conferences, and have received a lot of interest. blekko is a free service.*\r\n","score":113},{"title":"Beyond code: Versioning data with Git and Mercurial. ","id":346,"description":"* Stephanie Collett, California Digital Library, stephanie.collett@ucop.edu\r\n* Martin Haye, California Digital Library, martin.haye@ucop.edu \r\n\r\nWithin a relatively short time since their introduction, [distributed version control systems](http://en.wikipedia.org/wiki/Distributed_Version_Control_System) (DVCS) like [Git](http://git-scm.com/) and [Mercurial](http://mercurial.selenic.com/) have enjoyed widespread adoption for versioning code. It didn\u2019t take long for the library development community to start discussing the potential for using DVCS within our applications and repositories to version data. After all, many of the features that have made some of these systems popular in the open source community to version code (e.g. lightweight, file-based, compressed, reliable) also make them compelling options for versioning data.  And why write an entire versioning system from scratch if a DVCS solution can be a drop-in solution? At the [California Digital Library](http://www.cdlib.org/) (CDL) we\u2019ve started using Git and Mercurial in some of our applications to version data. This has proven effective in some situations and unworkable in others. This presentation will be a practical case study of CDL\u2019s experiences with using DVCS to version data. We will explain how we\u2019re incorporating Git and Mercurial in our applications, describe our successes and failures and consider the issues involved in repurposing these systems for data versioning.","score":285},{"title":"Design for Developers","id":347,"description":"    Lisa Kurt, University of Nevada, Reno, lkurt@unr.edu \r\n\r\nUsers expect good design. This talk will delve into what makes really great design, what to look for, and how to do it. Learn the principles of great design to take your applications, user interfaces, and projects to a higher level. With years of experience in graphic design and illustration, Lisa will discuss design principles, trends, process, tools, and development. Design examples will be from her own projects as well as a variety from industry. You\u2019ll walk away with design knowledge that you can apply immediately to a variety of applications and a number of top notch go-to resources to get you up and running. ","score":224},{"title":"Building research applications with Mendeley","id":348,"description":"William Gunn, Mendeley william.gunn@mendeley.com (@mrgunn)\r\n\r\nThis is partly a tool talk and partly a big idea one.\r\n\r\nMendeley has built the world's largest open database of research and we've now begun to collect some interesting social metadata around the document metadata. I would like to share with the Code4Lib attendees information about using this resource to do things within your application that have previously been impossible for the library community, or in some cases impossible without expensive database subscriptions. One thing that's now possible is to augment catalog search by surfacing information about content usage, allowing people to not only find things matching a query, but popular things or things read by their colleagues. In addition to augmenting search, you can also use this information to augment discovery. Imagine an online exhibit of artifacts from a newly discovered dig not just linking to papers which discuss the artifact, but linking to really good interesting papers about the place and the people who made the artifacts. So the big idea is, \"How will looking at the literature from a broader perspective than simple citation analysis change how research is done and communicated? How can we build tools that make this process easier and faster?\" I can show some examples of applications that have been built using the Mendeley and PLoS APIs to begin to address this question, and I can also present results from Mendeley's developer challenge which shows what kinds of applications researchers are looking for, what kind of applications peope are building, and illustrates some interesting places where the two don't overlap. ","score":181},{"title":"Your UI can make or break the application (to the user, anyway)","id":349,"description":"    Robin Schaaf, University of Notre Dame, schaaf.4@nd.edu \r\n\r\nUI development is hard and too often ends up as an after-thought to computer programmers - if you were a CS major in college I'll bet you didn't have many, if any, design courses. I'll talk about how to involve the users upfront with design and some common pitfalls of this approach. I'll also make a case for why you should do the screen design before a single line of code is written. And I'll throw in some ideas for increasing usability and attractiveness of your web applications. I'd like to make a case study of the UI development of our open source ERMS. ","score":163},{"title":"Why Nobody Knows How Big The Library Really Is - Perspective of a Library Outside Turned Insider","id":350,"description":"    Patrick Berry, California State University, Chico, pberry@csuchico.edu \r\n\r\nIn this talk I would like to bring the perspective of an \"outsider\" (although an avowed IT insider) to let you know that people don't understand the full scope of the library. As we \"rethink education\", it is incumbent upon us to help educate our institutions as to the scope of the library. I will present some of the tactics I'm employing to help people outside, and in some cases inside, the library to understand our size and the value we bring to the institution. ","score":102},{"title":"Building a URL Management Module using the Concrete5 Package Architecture","id":351,"description":"David Uspal, Villanova University, david.uspal@villanova.edu \r\n\r\nKeeping track of URLs utilized across a large website such as a university library, and keeping that content up to date for subject and course guides, can be a pain, and as an open source shop, we\u2019d like to have open source solution for this issue.  For this talk, I intend to detail our solution to this issue by walking step-by-step through the building process for our URL Management module -- including why a new solution was necessary; a quick rundown of our CMS ([Concrete5](http://www.concrete5.org), a CMS that isn\u2019t Drupal); utilizing the Concrete5 APIs to isolate our solution from core code (to avoid complications caused by core updates); how our solution was integrated into the CMS architecture for easy installation; and our future plans on the project.\r\n","score":94},{"title":"Building an NCIP connector to OpenSRF to facilitate resource sharing","id":352,"description":"Jon Scott, Lyrasis, jon_scott@wsu.edu and Kyle Banerjee, Orbis Cascade Alliance, banerjek@uoregon.edu \r\n\r\nHow do you reverse engineer any protocol to provide a new service? Humans (and worse yet, committees) often design verbose protocols built around use cases that don't line up current reality. To compound difficulties, the contents of protocol containers are not sufficiently defined/predictable and the only assistance available is sketchy documentation and kind individuals on the internet willing to share what they learned via trial by fire.\r\n\r\n\r\nNCIP (Niso Circulation Interchange Protocol) is an open standard that defines a set of messages to support exchange of circulation data between disparate circulation, interlibrary loan, and related applications -- widespread adoption of NCIP would eliminate huge amounts of duplicate processing in separate systems.\r\n\r\n\r\nThis presentation discusses how we learned enough about NCIP and OpenSRF from scratch to build an NCIP responder for Evergreen to facilitate resource sharing in a large consortium that relies on over 20 different ILSes. ","score":108},{"title":"Practical Agile: What's Working for Stanford, Blacklight, and Hydra","id":353,"description":"Naomi Dushay, Stanford University Libraries, ndushay@stanford.edu \r\n\r\nAgile development techniques can be difficult to adopt in the context of library software development. Maybe your shop has only one or two developers, or you always have too many simultaneous projects. Maybe your new projects can\u2019t be started until 27 librarians reach consensus on the specifications.\r\n\r\nThis talk will present successful Agile- and Silicon-Valley-inspired practices we\u2019ve adopted at Stanford and/or in the Blacklight and Hydra projects. We\u2019ve targeted developer happiness as well as improved productivity with our recent changes. User stories, dead week, sight lines \u2026 it\u2019ll be a grab bag of goodies to bring back to your institution, including some ideas on how to adopt these practices without overt management buy in. ","score":228},{"title":"Quick and <del>Dirty</del> Clean Usability: Rapid Prototyping with Bootstrap","id":354,"description":"Shaun Ellis, Princeton University Libraries, shaune@princeton.edu \r\n\r\n*\"The code itself is unimportant; a project is only as useful as people actually find it.*  - Linus Torvalds'' [http://bit.ly/p4uuyy](http://bit.ly/p4uuyy)\r\n\r\nUsability has been a buzzword for some time now, but what is the process for making the the transition toward a better user experience, and hence, better designed library sites?  I will discuss the one facet of the process my team is using to redesign the Finding Aids site for Princeton University Libraries (still in development).  The approach involves the use of rapid prototyping, with Bootstrap [http://twitter.github.com/bootstrap/], to make sure we are on track with what users and stakeholders expect up front, and throughout the development process.\r\n\r\nBecause Bootstrap allows for early and iterative user feedback, it is more effective than the historic Photoshop mockups/wireframe technique.  The Photoshop approach allows stakeholders to test the look, but not the feel -- and often leaves developers scratching their heads.  Being a CSS/HTML/Javascript grid-based framework, Bootstrap makes it easy for anyone with a bit of HTML/CSS chops to quickly build slick, interactive prototypes right in the browser -- tangible solutions which can be shared, evaluated, revised, and followed by all stakeholders (see [Minimum Viable Products](http://en.wikipedia.org/wiki/Minimum_viable_product)).  Efficiency is multiplied because the customized prototypes can flow directly into production use, as is the goal with iterative development approaches, such as the Agile methodology.\r\n\r\nWhile Bootstrap is not the only framework that offers grid-based layout, development is expedited and usability is enhanced by Bootstraps use of of \"prefabbed\" conventional UI patterns, clean typography, and lean Javascript for interactivity.   Furthermore, out-of-the box Bootstrap comes in a fairly neutral palette, so focus remains on usability, and does not devolve into premature discussions of color or branding choices.  Finally, using Less can be a powerful tool in conjunction with Bootstrap, but is not necessary.  I will discuss the pros and cons, and offer examples for how to getting up and running with or without Less.","score":178},{"title":"Search Engine Relevancy Tuning - A Static Rank Framework for Solr/Lucene","id":355,"description":"Mike Schultz, Amazon.com (formerly Summon Search Architect) mike.schultz@gmail.com \r\n\r\nSolr/Lucene provides a lot of flexibility for adjusting relevancy scoring and improving search results. Roughly speaking there are two areas of concern: Firstly, a 'dynamic rank' calculation that is a function of the user query and document text fields. And secondly, a 'static rank' which is independent of the query and generally is a function of non-text document metadata. In this talk I will outline an easily understood, hand-tunable static rank system with a minimal number of parameters.\r\n\r\nThe obvious major feature of a search engine is to return results relevant to a user query. Perhaps less obvious is the huge role query independent document features play in achieving that. Google's PageRank is an example of a static ranking of web pages based on links and other secret sauce. In the Summon service, our 800 million documents have features like publication date, document type, citation count and Boolean features like the-article-is-peer-reviewed. These fields aren't textual and remain 'static' from query to query, but need to influence a document's relevancy score. In our search results, with all query related features being equal, we'd rather have more recent documents above older ones, Journals above Newspapers, and articles that are peer reviewed above those that are not. The static rank system I will describe achieves this and has the following features:\r\n\r\n* Query-time only calculation - nothing is baked into the index - with parameters adjustable at query time.\r\n* The system is based on a signal metaphor where components are 'wired' together. System components allow multiplexing, amplifying, summing, tunable band-pass filtering, string-to-value-mapping all with a bare minimum of parameters.\r\n* An intuitive approach for mixing dynamic and static rank that is more effective than simple adding or multiplying.\r\n* A way of equating disparate static metadata types that leads to understandable results ordering. ","score":203},{"title":"Submitting Digitized Book-like things to the Internet Archive","id":356,"description":"Joel Richard, Smithsonian Institution Libraries, richardjm@si.edu \r\n\r\nThe Smithsonian Libraries has submitted thousands of out-of-copyright items to the Internet Archive over the years. Specifically in relation to the Biodiversity Heritage Library, we have developed an in-house boutique scanning and upload process that became a learning experience in automated uploading to the Archive. As part of the software development, we created a whitepaper that details the combined learning experiences of the Smithsonian Libraries and the Missouri Botanical Garden. We will discuss some of the the contents of this whitepaper in the context of our scanning process and the manner in which we upload items to the Archive.\r\n\r\nOur talk will include a discussion of the types of files and their formats used by the Archive, processes that the Archive performs on uploaded items, ways of interacting and affecting those processes, potential pitfalls and solutions that you may encounter when uploading, and tools that the Archive provides to help monitor and manage your uploaded documents.\r\n\r\nFinally, we'll wrap up with a brief summary of how to use things that are on the Internet Archive in your own websites.\r\n","score":144},{"title":"So... you think you want to Host a Code4Lib National Conference, do you? ","id":357,"description":"Elizabeth Duell, Orbis Cascade Alliance, eduell@uoregon.edu \r\n\r\nAre you interested in hosting your own Code4Lib Conference? Do you know what it would take? What does BEO stands for? What does F&B Minimum mean? Who would you talk to for support/mentoring? There are so many things to think about: internet support, venue size, rooming blocks, contracts, dietary restrictions and coffee (can't forget the coffee!) just to name a few. Putting together a conference of any size can look daunting, so let's take the scary out of it and replace it with a can do attitude!\r\n\r\nBe a step ahead of the game by learning from the people behind the curtain. Ask questions and be given templates/ cheat sheets!\r\n","score":82},{"title":"HTML5 Microdata and Schema.org","id":358,"description":"Jason Ronallo, North Carolina State University Libraries, jason_ronallo@ncsu.edu \r\n\r\nWhen the big search engines announced support for HTML5 microdata and the schema.org vocabularies, the balance of power for semantic markup in HTML shifted. \r\n\r\n* What is microdata? \r\n\r\n* Where does microdata fit with regards to other approaches like RDFa and microformats? \r\n\r\n* Where do libraries stand in the worldview of Schema.org and what can they do about it? \r\n\r\n* How can implementing microdata and schema.org optimize your sites for search engines?\r\n\r\n* What tools are available?\r\n\r\n","score":291},{"title":"Stack View: A Library Browsing Tool","id":359,"description":"Annie Cain, Harvard Library Innovation Lab, acain@law.harvard.edu \r\n\r\nIn an effort to recreate and build upon the traditional method of browsing a physical library, we used catalog data, including dimensions and page count, to create a [virtual shelf](http://librarylab.law.harvard.edu/projects/stackview/).\r\n\r\nThis CSS and JavaScript backed visualization allows items to sit on any number of different shelves, really taking advantage of its digital nature.  See how we built Stack View on top of our data and learn how you can create shelves of your own using our open source code.","score":172},{"title":"\"Linked-Data-Ready\" Software for Libraries","id":360,"description":"Jennifer Bowen, University of Rochester River Campus Libraries, jbowen@library.rochester.edu \r\n\r\nLinked data is poised to replace MARC as the basis for the new library bibliographic framework. For libraries to benefit from linked data, they must learn about it, experiment with it, demonstrate its usefulness, and take a leadership role in its deployment.\r\n\r\nThe eXtensible Catalog Organization (XCO) offers open-source software for libraries that is \u201clinked-data-ready.\u201d XC software prepares MARC and Dublin Core metadata for exposure to the semantic web, incorporating FRBR Group 1 entities and registered vocabularies for RDA elements and roles. This presentation will include a software demonstration, proposed software architecture for creation and management of linked data, a vision for how libraries can migrate from MARC to linked data, and an update on XCO progress toward linked data goals. ","score":256},{"title":"How people search the library from a single search box ","id":361,"description":"Cory Lown, North Carolina State University Libraries, cory_lown@ncsu.edu \r\n\r\nSearching the library is complex. There's the catalog, article databases, journal title and database title look-ups, the library website, finding aids, knowledge bases, etc. How would users search if they could get to all of these resources from a single search box? I'll share what we've learned about single search at NCSU Libraries by tracking use of QuickSearch ([http://www.lib.ncsu.edu/search/index.php?q=aerospace+engineering](http://www.lib.ncsu.edu/search/index.php?q=aerospace+engineering)), our home-grown unified search application. As part of this talk I will suggest low-cost ways to collect real world use data that can be applied to improve search. I will try to convince you that data collection must be carefully planned and designed to be an effective tool to help you understand what your users are telling you through their behavior. I will talk about how the fragmented library resource environment challenges us to provide useful and understandable search environments. Finally, I will share findings from analyzing millions of user transactions about how people search the library from a production single search box at a large university library.","score":262},{"title":"An Incremental Approach to Archival Description and Access ","id":362,"description":"*  Chela Scott Weber, New York University Libraries, chelascott@gmail.com\r\n*  Mark A. Matienzo, Yale University Library, mark@matienzo.org \r\n\r\nAn ongoing problem for many archives and special collections units is a lack of technological infrastructure and ongoing support. Funding for many archival programs arrives on a project-by-project basis, often in the form of grants. One of the largest concerns for archivist, therefore, is ensuring the sustainability of any solutions or processes that support core operations, such as archival description and access systems.\r\n\r\nThe presenters will describe their experience developing an iterative and sustainable approach to archival description and access at the library of a small historical society. Starting with mostly OCRed legacy finding aids and no online access to collections, and ending with structured data about the entirety of their holdings available online over three years time, we will detail the evolution of the work from problem-solving through to the resulting phases of descriptive work and development of a basic online access portal created in WordPress. We will discuss making reasonable and sustainable choices in an environment with little monetary and technical support, and how the organization's staff were able to build a system and processes that could leverage messy legacy metadata initially and grow to use structured, standardized data as it was created. We will also discuss the specific technical solutions we developed (the WordPress instance and supporting plugins) and our experience with how bugs and barriers outside of our control changed our insights. ","score":141},{"title":"Making the Easy Things Easy: A Generic ILS API ","id":363,"description":"Wayne Schneider, Hennepin County Library, wschneider@hclib.org \r\n\r\nSome stuff we try to do is complicated, because, let's face it, library data is hard. Some stuff, on the other hand, should be easy. Given an item identifier, I should be able to look at item availability. Given a title identifier, I should be able to place a request. And no, I shouldn't have to parse through the NCIP specification or write a SIP client to do it.\r\n\r\nThis talk will present work we have done on a web services approach to an API for traditional library transactional data, including example applications. ","score":106},{"title":"Your Catalog in Linked Data","id":364,"description":"Tom Johnson, Oregon State University Libraries, thomas.johnson@oregonstate.edu \r\n\r\nLinked Library Data activity over the last year has seen bibliographic data sets and vocabularies proliferating from traditional library sources. We've reached a point where regular libraries don't have to go it alone to be on the Semantic Web. There is a quickly growing pool of things we can actually link to, and everyone's existing data can be immediately enriched by participating.\r\n\r\nThis is a quick and dirty road to getting your catalog onto the Linked Data web. The talk will take you from start to finish, using Free Software tools to establish a namespace, put up a SPARQL endpoint, make a simple data model, convert MARC records to RDF, and link the results to major existing data sets (skipping conveniently over pesky processing time). A small amount of \"why linked data?\" content will be covered, but the primary goal is to leave you able to reproduce the process and start linking your catalog into the web of data. Appropriate documentation will be on the web. ","score":214},{"title":"Getting the Library into the Learning Management System using Basic LTI ","id":365,"description":"David Walker, California State University, dwalker@calstate.edu\r\n\r\nThe integration of library resources into learning management systems (LMS) has long been something of a holy grail for academic libraries. The ability to deliver targeted library systems and services to students and faculty directly within their online course would greatly simplify access to library resources. Yet, the technical barriers to achieving that goal have to date been formidable.\r\n\r\nThe recently released Learning Tool Interoperability (LTI) protocol, developed by IMS, now greatly simplifies this process by allowing libraries (and others) to develop and maintain \u201ctools\u201d that function like a native plugin or building block within the LMS, but ultimately live outside of it. In this presentation, David will provide an overview of Basic LTI, a simplified subset (or profile) of the wider LTI protocol, showing how libraries can use this to easily integrate their external systems into any major LMS. He\u2019ll showcase the work Cal State has done to do just that. ","score":145},{"title":"Turn your Library Proxy Server into a Honeypot ","id":366,"description":"Calvin Mah, Simon Fraser University, calvinm@sfu.ca (@calvinmah) \r\n\r\nEzproxy has provided libraries with a useful tool for providing patrons with offsite online access to licensed electronic resources. This has not gone unnoticed for the unscrupulous users of the Internet who are either unwilling or unable to obtain legitimate access to these materials for themselves. Instead, they buy or share hacked university computing accounts for unauthorized access. When undetected, abuse of compromised university accounts can lead to abuse of vendor resources which lead to the blocking of the entire campus block of IP addresses from accessing that resource.\r\n\r\nSimon Fraser University Library has been pro actively detecting and thwarting unauthorized attempts through log analysis. Since SFU has begun analysing our ezproxy logs, the number of new SFU login credentials which are posted and shared in publicly accessible forums has been reduced to zero. Since our log monitoring began in 2008, the annual average number of SFU login credentials that are compromised or hacked is 140. Instead of being a single point of weakness in campus IT security, the library\u2019s proxy server is a honeypot exposing weak passwords, keystroke logging trojans installed on patron PCs and campus network password sniffers.\r\n\r\nThis talk will discuss techniques such as geomapping login attempts, strategies such as seeding phishing attempts and tools such as statistical log analysis used in detecting compromised login credentials. ","score":135},{"title":"Relevance Ranking in the Scholarly Domain ","id":367,"description":"Tamar Sadeh, PhD, Ex Libris Group, tamar.sadeh@exlibrisgroup.com \r\n\r\nThe greatest challenge for discovery systems is how to provide users with the most relevant search results, given the immense landscape of available content. In a manner that is similar to human interaction between two parties, in which each person adjusts to the other in tone, language, and subject matter, discovery systems would ideally be sophisticated and flexible enough to adjust their algorithms to individual users and each user\u2019s information needs.\r\n\r\nWhen evaluating the relevance of an item to a specific user in a specific context, relevance-ranking algorithms need to take into account, in addition to the degree to which the item matches the query, information that is not embodied in the item itself. Such information, which includes the item\u2019s scholarly value, the type of search that the user is conducting (e.g., an exploratory search or a known-item search), and other factors, enables a discovery system to fulfill user expectations that have been shaped by experience with Web search engines.\r\n\r\nThe session will focus on the challenges of developing and evaluating relevance-ranking algorithms for the scholarly domain. Examples will be drawn mainly from the relevance-ranking technology deployed by the Ex Libris Primo discovery solution. ","score":159},{"title":"Mobile Library Catalog using Z39.50 ","id":368,"description":"James Paul Muir, The Ohio State University, muir.29@osu.edu \r\n\r\nA talk about putting a new spin on an age-old technology, creating a universal interface, which exposes any Z39.50 capable library catalog as a simple, useful and universal REST API for use in native mobile apps and mobile web.\r\n\r\nThe talk includes the exploration and demonstration of the Ohio State University\u2019s native app \u201cOSU Mobile\u201d for iOS and Android and shows how the library catalog search was integrated.\r\n\r\nThe backbone of the project is a REST API, which was created in a weekend using a PHP framework that translates OPAC XML results from the Z39.50 interface into mobile-friendly JSON formatting.\r\n\r\nRaw Z39.50 search results contain all MARC information as well as local holdings. Configurable search fields and the ability to select which fields to include in the JSON output make this solution a perfect fit for any Z39.50-capable library catalog.\r\n\r\nLooking forward, possibilities for expansion include the use of Off Campus Sign-In for online resources so mobile patrons can directly access online resources from a smartphone (included in the Android version of OSU Mobile) as well as integration with library patron account.\r\n\r\nEnjoy this alternative to writing a custom OPAC adapter or using a 3rd party service for exposing library records and use the proven and universal Z39.50 interface directly against your library catalog. ","score":101},{"title":"DMPTool: Guidance and resources to build a data management plan ","id":369,"description":"Marisa Strong, California Digital Libary, marisa.strong@ucop.edu \r\n\r\nA number of U.S. funding agencies such as the National Science Foundation require researchers to supply detailed plans for managing research data, called Data Management Plans. To help researchers with this requirement, the California Digital Library (CDL) along with several organizations, collaborated to develop the DMPTool. The goal is to provide researchers with guidance, links to resources and help with writing data management plans.\r\n\r\nThis open-source, Ruby on Rails software tool is hosted on a SLES VM by CDL. The tool is integrated with Shibboleth, federated single sign-on software, which allows users to login via their home institutions. We had a geographically distributed development team sharing their code on Bitbucket.\r\n\r\nThis talk will demo features of the application, the Shibboleth login architecture, as well as highlight the agile development practices and methods used to successfully design and build the application on an aggressive schedule. ","score":116},{"title":"Lies, Damned Lies, and Lines of Code Per Day ","id":370,"description":"James Stuart, Columbia University, james.stuart@columbia.edu \r\n\r\nWe've all heard about that one study that showed that Pair Programming was 20% efficient than working alone. Or maybe you saw on a blog that study that showed that programmers who write fewer lines of code per day are more efficient...or was it less efficient? And of course, we all know that programmers who work in (Ruby|Python|Java|C|Erlang) have been shown to be more efficient.\r\n\r\nA quick examination of some of the research surrounding programming efficiency and methodology, with a focus on personal productivity, and how to incorporate the more believable research into your own team's workflow. ","score":162},{"title":"An Anatomy of a Book Viewer","id":371,"description":"Mohammed Abuouda, Bibliotheca Alexandrina, mohammed.abuouda@bibalex.org \r\n\r\nBibliotheca Alexandria (BA) hosts 210,000 digital books in different languages available at [http://dar.bibalex.org](http://dar.bibalex.org). It includes the largest collection of digitized Arabic books. Using open source tools, BA has developed a modular book viewer that can be deployed in any environment to provide the users with a great personalized reading experience. BA\u2019s book viewer provides several services that make this possible: morphological search in different languages, localization, server load balancing, scalability and image processing. Personalization features includes different types of annotation such as sticky notes, highlighting and underlining. It also provides the ability to embed the viewer in any webpage and change its skin.\r\n\r\nIn this talk we will describe the book viewer architecture, its modular design and how to incorporate it in your current environment.","score":184},{"title":"Carrier: Digital Signage System ","id":372,"description":"Justin Spargur, The University of Arizona, spargurj@u.library.arizona.edu \r\n\r\nCarrier is a web-based digital signage application written using JavaScript, PHP, MySQL that can be used on any device with an internet connection and a web browser. Used across the University of Arizona Libraries campuses, Carrier can display any web-based content, allowing users to promote new library collections and services via images, web pages, or videos. Users can easily manage the order in which slides are delivered, manage the length that slides are displayed for, set dates for when slides should be shown, and even specify specific locations where slides should be presented.\r\n\r\nIn addition to marketing purposes, Carrier can be used to send both low and high priority alerts to patrons. Alerts can be sent through the administrative interface, via RSS feeds, and even through a Twitter feed, allowing for easy integration with existing campus emergency notification systems.\r\n\r\nI will describe the technical underpinnings of Carrier, challenges that we\u2019ve faced since its implementation, enhancements planned for the next release of the software, and discuss our plans for releasing this software for others to use **for free**. ","score":104},{"title":"We Built It. They Came. Now What? ","id":373,"description":"Evviva Weinraub, Oregon State University, evviva.weinraub@oregonstate.edu \r\n\r\nYou have a great idea for something new or useful. You build it, put it out there on GitHub, do a couple of presentations, maybe a press release and BAM, suddenly you\u2019ve created a successful Open Source tool that others are using. Great!\r\n\r\nFast-forward 3 years.\r\n\r\nYou still believe in the product, but you can no longer be solely responsible for taking care of it. Just putting it out there has made it a tool others use, but how do you find a community of folks who believe in the product as much as you do and are willing to commit the time and energy into building, sustaining and moving this project forward. Or just figuring out if you should bother trying?\r\n\r\nIn 2006, OSU Libraries built an Interactive Course Assignment system called Library a la Carte \u2013 think LibGuides only Open Source. We now find ourselves in just this predicament.\r\n\r\nWhat can we do as a community to move beyond our build-first-ask-questions-later mentality and embed sustainability into our new and existing ideas and products without moving toward commercialization? I fully expect we\u2019ll end up with more questions than answers, but let\u2019s spend some talking about our predicament and yours and think about how we can come out the other side. ","score":149},{"title":"Contextually Rich Collections Without the Risk: Digital Forensics and Automated Data Triage for Digital Collections ","id":374,"description":"* Kam Woods, University of North Carolina at Chapel Hill, kamwoods@email.unc.edu\r\n* Cal Lee, University of North Carolina at Chapel Hill, callee -- at -- ils -- unc -- edu \r\n\r\nDigital libraries and archives are increasingly faced with a significant backlog of unprocessed data along with an accelerating stream of incoming material. These data often arrive from donor organizations, institutions, and individuals on hard drives, optical and magnetic disks, flash memory devices, and even complete hardware (traditional desktop computers and mobile systems).\r\n\r\nInformation on these devices may be sensitive, obscured by operating system arcana, or require specialized tools and procedures to parse. Furthermore, the sheer volume of materials being handled means that even simple tasks such as providing useful content reports can be impractical (or impossible) in current workflows.\r\n\r\nMany of the tasks currently associated with data triage and analysis can be simplified and performed with improved coverage and accuracy through the use of open source digital forensics tools. In this talk we will discuss recent developments in providing digital librarians and archivists with simple, open source tools to accomplish these tasks. We will discuss tools and methods be tested, developed and packaged as part of the BitCurator project. These tools can be used to reduce or eliminate laborious, error-prone tasks in existing workflows and put valuable time back into the hands of digital librarians and archivists -- time better used to identify and tackle complex tasks that *cannot* be solved by software. ","score":142},{"title":"Finding Movies with FRBR and Facets ","id":375,"description":"Kelley McGrath, University of Oregon, kelleym@uoregon.edu \r\n\r\nHow might the Functional Requirements for Bibliographic Records (FRBR) model and faceted navigation improve access to film and video in libraries? I will describe the design and implementation of a [FRBR-inspired prototype discovery interface](http://blazing-sunset-24.heroku.com/) using Solr and Blacklight . This approach demonstrates how FRBR can enable a work-centric view that is focused on the original movie or program while supporting users in selecting an appropriate version.\r\n\r\nThe prototype features two sets of facets, which independently address two important information needs: (1) \"What kind of movie or program do you want to watch?\" (e.g., a 1970s TV sitcom, something directed by Kurosawa, or an early German horror film); (2) \"How do you want to watch it? Where do you want to get it from?\" (e.g., on Blu-ray, with Spanish subtitles, available at the local public library). This structure enables patrons to narrow, broaden and pivot across facet values instead of limiting them to the tree-structured hierarchy common with existing FRBR applications.\r\n\r\nThis type of interface requires controlled data values mapped to FRBR group 1 entities, which in many cases are not available in existing MARC bibliographic records. I will discuss ongoing work using the [XC Metadata Services Toolkit](http://www.extensiblecatalog.org/) to extract and normalize data from existing MARC records for videos in order to populate a FRBRized, faceted discovery interface. ","score":132},{"title":"Escaping the Black Box \u2014 Building a Platform to Foster Collaborative Innovation","id":376,"description":"* Karen Coombs, OCLC, coombsk@oclc.org\r\n* Kathryn Harnish, OCLC harnishk@oclc.org \r\n\r\nExposed Web services offer an unprecedented opportunity for collaborative innovation \u2014 that\u2019s one of the hallmarks of Web-based services like Amazon, Google, and Facebook. These environments are popular not only for their native feature sets, but also for the array of community-developed apps that can run in them. The creativity of the development communities that work in these systems brings new value to all types of users.\r\n\r\nWhat if the library community could realize this same level of collaborative innovation around its systems? What kinds of support would be necessary to transform library systems from \u201cblack boxes\u201d to more open, accessible environments in which value is created and multiplied by the user community?\r\n\r\nIn this session, we\u2019ll discuss the challenges and opportunities OCLC faced in creating just that kind of environment. The recently-released OCLC \u201ccooperative platform\u201d provides improved access to a wide variety of OCLC\u2019s data and services, allowing library developers and other interested partners to collaborate, innovate, and share new solutions with fellow libraries. We\u2019ll describe the open standards and technologies we\u2019ve put in play in as we:\r\n\r\n* exposed robust Web services that provide access to both data and business logic;\r\n* created an architecture for integrating community-built applications in OCLC (and other) products; and\r\n* developed an infrastructure to support community development, collaboration, and app sharing \r\n\r\nLearn how OCLC is helping to open the \u201cblack box\u201d -- and give libraries the freedom to become true partners in the evolution of their library systems. ","score":115},{"title":"Code inheritance; or, The Ghosts of Perls Past ","id":377,"description":"Jon Gorman, University of Illinois, jtgorman@illinois.edu\r\n\r\nAny organization has a history not found in its archives or museums. Mysteries exist that origins are lost to the collective institutional knowledge. Despite what has been forgotten by humans, our servers and computers still keep running. Instructions crafted long ago execute like digital ghosts following orders of masters who have long since left.\r\n\r\nThe University of Illinois has a fair amount of Perl code created by several different developers. This code includes software that handles our data feeds coming both in and out of campus, reports against our Voyager system, some web applications, and more.\r\n\r\nI'll touch a little on the historical legacy and why Perl is used. From there I'll share some tips, best practices, and some of the mistakes I've made in trying to maintain this code. Most of the advice will transition to any language, but code and libraries discussed will be Perl. The presentation will also touch on some internal debate on whether or not to port parts of our Perl codebase.\r\n\r\n","score":118},{"title":"Recorded Radio/TV broadcasts streamed for library users ","id":378,"description":"* K\u00e5re Fiedler Christiansen, The State and University Library Denmark, kfc@statsbiblioteket.dk\r\n* Mads Villadsen, The State and University Library Denmark, mv@statsbiblioteket.dk \r\n\r\n\"Provide online access to the Radio/TV collection,\" my boss said. About 500,000 hours of Danish broacast radio and TV. Easy, right? Well, half a year later we'd done it, but it turned out to involve practically every it employee in the library and quite a few non-technical people as well.\r\n\r\nCombining our Fedora-based DOMS repository system with our Lucene-based Summa search system with our WAYF-based single-signon system with an upgrade of our SAN system for enough speed to deliver the content with an ffmpeg-based transcoding workflow system with a Wowza-based streaming server, and sprinkling it all with a nice user-friendly web frontend turned out to be quite a challenge, but also one of the most engaging experiences for a long time.\r\n\r\nOf course we were immidiately shut down, since the legal details weren't quite as clear as we thought they were, but take an exclusive preview at [http://developer.statsbiblioteket.dk/kultur/](http://developer.statsbiblioteket.dk/kultur/) - username/password: code4lib","score":137},{"title":"NoSQL Bibliographic Records: Implementing a Native FRBR Datastore with Redis ","id":379,"description":"Jeremy Nelson, Colorado College, jeremy.nelson@coloradocollege.edu \r\n\r\nIn October, the Library of Congress issued a news release, \"A Bibliographic Framework for the Digital Age\" outlining a list of requirements for a New Bibliographic Framework Environment. Responding to this challenge, this talk will demonstrate a [Redis](http://redis.io) FRBR datastore proof-of-concept that, with a lightweight python-based interface, can meet these requirements.\r\n\r\nBecause FRBR is an Entity-Relationship model; it is easily implemented as key-value within the primitive data structures provided by Redis. Redis' flexibility makes it easy to associate arbitrary metadata and vocabularies, like MARC, METS, VRA or MODS, with FRBR entities and inter-operate with legacy and emerging standards and practices like RDA Vocabularies and LinkedData. ","score":167},{"title":"Upgrading from Catalog to Discovery Environment: A Consortial Approach ","id":380,"description":"* Spencer Lamm, Swarthmore College, slamm1@swarthmore.edu\r\n* Chelsea Lobdell, Swarthmore College, clobdel1@swarthmore.edu \r\n\r\nAlmost two years ago the Tri-College Consortium of Haverford, Swarthmore, and Bryn Mawr Colleges embarked upon a journey to provide enhanced end-user experience and discoverability with our library applications. Our solution was to implement an integration of ExLibris's Primo Central into Villanova's VuFind for a dual-channel searching experience. We present a case study of the collaborative and technical aspects of our process.\r\n\r\nAt a high level we will describe our approach to project management and decision making. We used a multi-tiered structure of working groups with an iterative design-feedback implementation cycle. We will relay lessons learned from our experience: successes, failures, and unexpected hurdles.\r\n\r\nAt a lower, technical level we will discuss the vufind search module architecture; the workflow of creating a new search channel; a Primo API parser; and the data structures of the Primo API response and the Primo SearchObject. Time permitting, we will also outline how we modified VuFind's Innovative driver to work with our ILS. ","score":107},{"title":"Improving geospatial data access for researchers and students ","id":381,"description":"* Dileshni Jayasinghe, Scholars Portal, University of Toronto, d.jayasinghe@utoronto.ca\r\n* Sepehr Mavedati, Scholars Portal, University of Toronto, sepehr.mavedati@utoronto.ca \r\n\r\n[Scholars GeoPortal](http://geo.scholarsportal.info) was created as a platform for online delivery of geospatial data resources to the Ontario Council of University Libraries community. Prior to the start of this project, each institution was storing data locally, and had its own practice for distributing datasets to users. This ranged from home grown online data delivery systems to burning data on to DVDs for each individual request. Most institutions had limited resources and expertise to create and maintain a sophisticated delivery system on their own. Led by OCUL Map, GIS librarians, staff at Scholars Portal in partnership with the Government of Ontario, the GeoPortal project began in 2009.\r\n\r\nOur talk will focus on the design and architecture of Scholars Portal's solution to support maps and geospatial data, and how we distribute these data collections to our users.\r\n\r\nThe system consists of 4 main components: metadata management system, map server, spatial database, and the web application.\r\n\r\n* Metadata Management: customized metadata editor with data hosted in MarkLogic, providing text and spatial queries\r\n* Map Server: ArcGIS Server\r\n* Spatial database: MS SQL Server with spatial extension\r\n* Web application: Javascript web application using Dojo and Esri\u2019s Javascript API \r\n\r\nFor other code4libbers who are interested in a similar system, we will also discuss the open source alternatives for each component (GeoNetwork, MapServer, etc.), and challenges and limitations we faced trying to use some of these tools. We'd also like to pick your brains on how we can make this application better. What can we do differently? ","score":137},{"title":"LibX 2.0 ","id":382,"description":"Godmar Back, Virginia Tech, godmar@gmail.com \r\n\r\nWe would like to provide the Code4Lib community with an update on what we've accomplished with LibX (which we last presented in 2009) - where we've gone, what our users are thinking, and how both its technology and its adapter community can be included in the code4lib world. We've grown to our 200,00 users, have a sleek, newly designed user interface, support for Google Chrome. We're now directly consuming many web services. Our Libapp Builders allows anyone to place results, cue, tutorials and other library-related information into pages. ","score":98},{"title":"Introducing the DuraSpace Incubator ","id":383,"description":"Jonathan Markow, DuraSpace, jjmarkow@duraspace.org \r\n\r\nDuraSpace is planning to launch a new incubation program for the benefit of open source projects that wish to become part of our organization, in the interest of helping them to become sustainable, community-driven projects and supporting them afterwards with umbrella services that help them to thrive. From time to time DuraSpace becomes aware of open source software projects in the preservation, archiving, or repository space that are in search of a community \u201chome\u201d. The motivation might be that the project is simply trying to attract more developers, that it would like to develop a more robust community of users and service providers, that its current organizational sponsorship is in question, or that it would like to take advantage of an existing and compatible organization's best practices and administrative infrastructure rather than create a new one of its own. DuraSpace is now prepared to leverage its resources, experience, and reputation in the community to help these projects become, or continue to be, successful. Projects emerging from incubation will become officially recognized as DuraSpace projects. This briefing presents highlights of the DuraSpace Incubator and invites questions and feedback from participants. ","score":85},{"title":"In-browser data storage and me ","id":384,"description":"Jason Casden, North Carolina State University Libraries, jason_casden@ncsu.edu \r\n\r\nWhen it comes to storing data in web browsers on a semi-persistent basis, there are several partially-adopted, semi-deprecated, product-specific, or even universally accepted options. These include models such as key-value stores, relational databases, and object stores. I will present some of these options and discuss possible applications of these technologies in library services. In addition to quoting heavily from Mark Pilgrim's excellent chapter on this topic, I will weave in my own experience utilizing in-browser data storage in an iPad-based data collection tool to successfully improve performance and data stability while reducing network dependence. See also: HTML5. ","score":177},{"title":"Coding for the past, archiving for the future \u2026 and the Salman Rushdie Papers ","id":385,"description":"Peter Hornsby, Emory University Libraries, phornsb@emory.edu \r\n\r\nCultural heritage production is moving to the digital medium and libraries use of repository solutions such as Fedora Commons and DSpace are a solid response to this change. But how do we go from, for instance a selection of 90's computing technology to a collection of digital objects ready for ingest into your institution's local repository? Once you have ingested your digital objects how are you going to provide access to these resources? The arrival of the Salman Rushdie Papers, which contain 10 years of Sir Salman Rushdie's digital life, gave Emory University Libraries the opportunity to explore these questions. I would like to to talk about the approach the Emory University Libraries adopted, what we learned and the coding challenges that remain. ","score":133},{"title":"Indexing big data with Tika, Solr & map-reduce ","id":386,"description":"* Scott Fisher, California Digital Library, scott.fisher AT ucop BORK edu\r\n* Erik Hetzner, California Digital Library, erik.hetzner AT ucop BORK edu \r\n\r\nThe Web Archiving Service at the California Digital Library has crawled a large amount of data, in every format found on the web: 30 TB, comprising about 600 million fetched URLs. In this talk we will discuss how we parsed this data using Tika and map-reduce, and how we indexed this data with Solr, tweaked the relevance ranking, and were able to provide our users with a better search experience. ","score":210},{"title":"ALL TEH METADATAS! or How we use RDF to keep all of the digital object metadata formats thrown at us.","id":387,"description":"Declan Fleming, University of California, San Diego, dfleming AT ucsd DING edu \r\n\r\n\r\nWhat's the right metadata standard to use for a digital repository?  There isn't just one standard that fits documents, videos, newspapers, audio files, local data, etc.  And there is no standard to rule them all.  So what do you do?  At UC San Diego Libraries, we went down a conceptual level and attempted to hold every piece of metadata and give each holding place some context, hopefully in a common namespace.  RDF has proven to be the ideal solution, and allows us to work with MODS, PREMIS, MIX, and just about anything else we've tried.  It also opens up the potential for data re-use and authority control as other metadata owners start thinking about and expressing their data in the same way.  I'll talk about our workflow which takes metadata from a stew of various sources (CSV dumps, spreadsheet data of varying richness, MARC data, and MODS data), normalizes them into METS by our Metadata Specialists who create an assembly plan, and then ingests them into our digital asset management system.  The result is a [beautiful graph](http://dl.dropbox.com/u/6923768/Work/DAMS%20object%20rdf%20graph.png) of RDF triples with metadata poised to be expressed as [HTML](https://libraries.ucsd.edu/digital/), RSS, METS, XML, and opens linked data possibilities that we are just starting to explore.\r\n","score":272},{"title":"HathiTrust Large Scale Search: Scalability meets Usability ","id":388,"description":"Tom Burton-West, DLPS, University of Michigan Library, tburtonw AT umich edu\r\n\r\n[HathiTrust Large-Scale search]([http://www.hathitrust.org/) provides full-text search services over  nearly 10 million full-text books using Solr for the back-end.  Our index is around 5-6 TB in size and each shard contains over 3 billion unique terms due to content in over 400 languages and dirty OCR.\r\n\r\nSearching the full-text of 10 million books often results in very large result sets.  By conference time a number of [http://www.hathitrust.org/full-text-search-features-and-analysis features] designed to help users narrow down large result sets and to do exploratory searching will either be in production or in preparation for release. There are often trade-offs between implementing desirable user features and keeping response time reasonable in addition to the traditional search trade-offs of precision versus recall.  \r\n\r\nWe will discuss various [scalability](http://www.hathitrust.org/blogs/large-scale-search) and usability issues including:\r\n* Trade-offs between desirable user features and keeping response time reasonable and scalable \r\n* Our solution to providing the ability to search within the 10 million books and also search within each book\r\n* Migrating the [http://babel.hathitrust.org/cgi/mb personal collection builder application] from a separate Solr instance to an app which uses the same back-end as full-text search.\r\n* Design of a scalable multilingual spelling suggester\r\n* Providing advanced search features combining MARC metadata with OCR\r\n** The dismax mm and tie parameters\r\n** Weighting issues and tuning relevance ranking\r\n* Displaying  only the most \"relevant\" facets\r\n* Tuning relevance ranking \r\n* Dirty OCR issues\r\n* CJK tokenizing and other multilingual issues.\r\n","score":238},{"title":"The Islandora Open Source Framework for Digital Asset Management ","id":390,"description":"Keith Folsom, Orbis Cascade Alliance, kfolsom@uoregon.edu \r\n\r\nManaging digital content is a challenging task\u2014becoming even more so as the volumes and types of content increase at what seems an exponential rate. Though there are good commercial management systems available, having competing and potentially more configurable open source options is ideal. One such option is Islandora\u2014an open source framework that wraps a Drupal front-end around the Fedora digital object management and storage system.\r\n\r\nMy talk will serve as an introduction to the Islandora framework\u2014including a discussion of Fedora\u2019s digital object model and content model architecture; how Islandora exposes the power of Fedora for storage, discovery, and retrieval of data; and the wide variety of underlying open source software and technology that enables the system. I will also give a quick tour of a stock Islandora installation and provide tips on navigating the documentation for set-up and use of this powerful framework. ","score":150},{"title":"What do the NISO IOTA OpenURL quality reports tell us about the future of OpenURL linking? ","id":391,"description":"Adam Chandler, Cornell University, alc28@cornell.edu \r\n\r\n[NISO IOTA](http://openurlquality.niso.org/) is an initiative that makes use of log files from various institutions and vendors to analyze element frequency and patterns contained within OpenURL requests. The reports created from this analysis inform vendors about where to make improvements to their OpenURLs. In this talk, the chair of the IOTA working group will share what the group has learned about the differences in quality across OpenURL sources. ","score":104},{"title":"\"CALIL.JP\" Open Libraries by web-scraping. - Introducing Library API from Japan ","id":392,"description":"Ryuuji Yoshimoto, Nota Inc. Engineer, ryuuji@notaland.com \r\n\r\nI am an engineer at Nota Inc., a start-up company for web services. [\"CALIL\"](http://calil.jp/) is a web service for library users in Japan. (Not only for librarians but also for general patrons.)\r\n\r\nCALIL allows users search books from multiple libraries nearby, and get realtime holding status. Our service supports over 5,800 libraries. CALIL supports public, university, and other many special libraries in Japan. The service can search 88% of collections of all public libraries in Japan. Public libraries in Japan do not have an unified catalogue like OCLC. Web OPACs in Japan are generally very slow and their usability is low. We develop a comprehensive scraping service over 2000 web OPACs and it supports recognize real-time holding status on them as well. This service can be used as for substitution of OPACs provided by libraries. It provides more useful, speedy and open service.\r\n\r\nOur scraping platform also provides API for free. Any developer can access realtime holding status at almost all the libraries in Japan by one API. Since the launch in 2010, many apps on iPhone and Android are developed by many third party developers. And it allows many web service connect to library (book shelf, review etc).\r\n\r\nCALIL is written by 100% pure Python and running on Google App Engine.\r\n\r\nI will introduce about \"CALIL\", \"CALIL Library API\", and its methodology. Open Libraries in Japan to World-Coders!! ","score":110},{"title":"Discovering Digital Library User Behavior with Google Analytics","id":393,"description":"Kirk Hess, Digital Humanities Specialist, University of Illinois Urbana-Champaign, kirkhess@illinois.edu \r\n\r\nDigital library administrators are frequently asked questions like \"How many times was that document downloaded\", or \"What\u2019s the most popular book in our collection?\" Conventional web logging software, such as AWStats, can only answer those questions some of the time, and there\u2019s always the question of whether or not the data is polluted by non-users, such as spiders and crawlers. [Google Analytics](http://google.com/analytics/ ), a JavaScript-based solution that excludes most crawlers and bots, shows how users found your site and how they explored it.\r\n\r\nThe presentation will review tracking search queries, adding events such as clicking external links or downloading files, and custom variables, to track user behavior that is normally difficult to track. We'll also discuss using jQuery scripts to add tracking code to the page without having to modify the underlying web application. Once you've collected data, you may use the Google Analytics API to extract data and integrate it with data from your digital repository to show granular data about individual items in your Digital Library. Finally, we'll discuss how this information allows you to improve the user experience, and summarize some of the research we are doing with our digital repository and the data gathered from Google Analytics. ","score":172},{"title":"Introducing Kuali OLE 0.3","id":394,"description":"* Rich Slabach, Quality Assurance Manager, Kuali OLE, rlslabac at indiana dot edu\r\n* Nianli Ma, Technical Architect, Kuali OLE, Indiana University, nianma at indiana dot edu \r\n\r\nThis research update will feature technical staff from the Kuali Open Library Environment (OLE) project, which is in its second year of building a community-source library management environment. Operating since July 2010, and supported by The Andrew W. Mellon Foundation, Kuali OLE is the one of the largest academic library software collaborations in the United States. In this presentation we will discuss the Kuali OLE Year 2 Roadmap as well as key components of the system architecture, additionally we will demonstrate our Kuali OLE 0.3 release from November 2011 with our cloud-based test drive implementation and our well documented driver's manual. This will lead to a better understanding of how this code base could support library management at your home institution.\r\n\r\nWe will also discuss opportunities for engagement with Kuali OLE and for adoption and use of the software as well as hear more about our plans for long-term sustainability. For more on our current software see - [https://wiki.kuali.org/display/OLE/OLE+and+Docstore+Server+Installation](https://wiki.kuali.org/display/OLE/OLE+and+Docstore+Server+Installation)","score":114},{"title":"UDFR: Building a Registry using Open-Source Semantic Software ","id":395,"description":"* Stephen Abrams, Associate Director, UC3, California Digital Library, stephen.abrams AT ucop DING edu\r\n* Lisa Dawn Colvin, UDFR Project Manager, California Digital Library, lisa.colvin AT ucop DING edu \r\n\r\nFundamental to effective long-term preservation analysis, planning, and intervention is the deep understanding of the diverse digital formats used to represent content. The Unified Digital Format Registry project ([UDFR](https://bitbucket.org/udfr/main/wiki/Home)) will provide an open source platform for an online, semantically-enabled registry of significant format representation information.\r\n\r\nWe will give an introduction to the UDFR tool and its use within a preservation process.\r\n\r\nWe will also discuss our experiences of integrating disparate data sources and models into RDF: describing our iterative data modeling process and decisions around integrating vocabularies, data sources and provenance representation.\r\n\r\nFinally, we will share how we extended an existing open-source semantic wiki tool, OntoWiki, to create the registry. ","score":87},{"title":"Sirsi Symphony: Developing a \"web service\" to provide real time bibliographic information to Blacklight.","id":396,"description":"John Pillans, Enterprise Software, Library Systems, Configuration Manager Kuali OLE, Indiana University, jpillan@indiana.edu \r\n\r\nIndiana University Libraries is currently in the process of implementing Blacklight as its discovery layer on top of Sirsi Symphony. One aspect of Blacklight that must be developed locally is providing circulation status and holdings information to the user. We have developed a \"web service\" which provides the bibliographic data, formatted MARC holdings data (if present), and item data with current circulation information to the Blacklight system in XML. ","score":79},{"title":"Open Sourcing the Dream: Making the Read/Write Library ","id":397,"description":"Margaret Heller, Read/Write Library Chicago and Dominican University, mheller@dom.edu \r\n\r\nYou met the Chicago Underground Library last year, now meet The Read/Write Library Chicago.It's a new name, a new space, and new opportunities to develop our catalog. We are working on creating the open source version of our ideas with a distributed team of interested volunteers, plus experimenting with innovative partnerships with the Chicago technology community. This talk will share what the team and open source project look like, what we are doing with our data, and how we finally learned to stop worrying and love Git. ","score":137},{"title":"Interactive maps: an easy-to-maintain and scalable approach ","id":398,"description":"Mariela Gunn, Oakland University, gunn@oakland.edu \r\n\r\nDeveloping interactive maps of a library building presents a unique challenge in an institution with limited web services personnel. Our technical expectations are high: we want the maps to have engaging interactivity, to be modular so we can link to different services represented in them, and to be scalable so that we can integrate data-driven elements. Our content needs are ever changing: we want to have distributed authorship of content through a user-friendly interface that can be used by all librarians without a steep learning curve.\r\n\r\nThis talk will focus on the design of interactive maps by a group of our undergraduate student interns who selected a web application -- Maps Alive -- for the task with ease of use and scalability in mind and set up a structure that can grow and change. The pros and cons of the application will be discussed, as well as tips on how to evaluate potential tools and make the best use of them through a modular and flexible approach to interactive maps. Involving students as designers and decision-makers in technology-related projects will be highlighted too. ","score":122},{"title":"Getting the Content out of CONTENTdm: Building a Modular UI Template for Digital Collections ","id":399,"description":"Devin Becker, University of Idaho, dbecker@uidaho.edu \r\n\r\nWith the advent of iterations 6 and 6.1, CONTENTdm redesigned the basic user interfaces for individual collections, improving on what was already a robust and reliable system for archiving and displaying digital items. The majority of the items in these collections, however, still rarely see the light of a user's screen. Moreover, the typical modes for browsing these collections within the system are geared primarily to those who are already familiar with such systems or who have a specific need to see certain items.\r\n\r\nTo invite more casual browsing and easier discovery of our collections, the University of Idaho Library's Digital Initiatives department designed a scalable and modular interface for all of our collections with an increased emphasis on the time, location, and larger display of our images and other digital items. To do so, we used free and easy-to-use Javascript libraries and online applications (including Jquery, Google Fusion Tables, Simile Timeline, ImageFlow, and Tagcrowd.com), together with several, simple XSL stylesheets that utilize the metadata and persistent linking capabilities of the CONTENTdm database, to design a basic template with several browsing options (timeline, map, tag cloud, etc.) that can be used for any collection. This talk will detail the coding, methods, and metadata implemented for the redesign. ","score":134},{"title":"saveMLAK: How Librarians, Curators, Archivists and Library Engineers Work Together with Semantic MediaWiki after the Great Earthquake of Japan ","id":400,"description":"* Yuka Egusa, Senior Researcher of National Institute of Educational Policy Research, yuka_at_nier.go.jp\r\n* Makoto Okamoto, Chief Editor of Academic Resource Guide (ARG), arg.editor_at_gmail.com \r\n\r\nIn March 11th 2011, the biggest earthquake and tsunami in the history attacked a large area of northern east region of Japan. A lot of people have worked together to save people in the area. For library community, a wiki named \"savelibrary\" was launched for sharing information on damages and rescues on the next day of the earthquake. Later then people from museum curators, archivists and community learning centers started similar projects. In April we joined to a project \"saveMLAK\", and launched a wiki site using Semantic MediaWiki under [http://savemlak.jp/](http://savemlak.jp/).\r\n\r\nAs of November 2011, information on over 13,000 cultural organizations are posted on the site by 269 contributors since the launch. The gathered information are organized along with Wiki categories of each type of facilities such library, museum, school, etc. We have held eight edit-a-thons to encourage people to contribute to the wiki.\r\n\r\nWe will report our activity, how the libraries and museums were damaged and have been recovered with lots of efforts, and how we can do a new style of collaboration with MLAK community, Wiki and other voluntary communities at the crisis. ","score":126},{"title":"Kill the search button II - the handheld devices are coming ","id":401,"description":"* J\u00f8rn Th\u00f8gersen, Statsbiblioteket/State and University Library, Aarhus, Denmark. jt@statsbiblioteket.dk\r\n* Michael Poltorak Nielsen, Statsbiblioteket/State and University Library, Aarhus, Denmark. mn@statsbiblioteket.dk, (aka the Danes - some of them).\r\n\r\nWeb based library search engines are traditionally operated using keys, input fields, buttons, and links. Being equipped with touch screens, accelerometers, GPS's, and cameras, smartphones and tablets offer a whole new range of input options.\r\n\r\nIn this talk we'll demonstrate some of our ideas of how to\r\nutilise these new input options interacting with a search engine. The basic idea is to have no traditional GUI input elements, but only use touch interactions (pinch, zoom, swipe, long-press, etc) and gestures (shake, tilt, turn, etc.). Using these interactions, we\u2019ll demonstrate how to:\r\n\r\n* do searches\r\n* toggle search result views\r\n* switch pages\r\n* request materials, add to favourites\r\n* interact with your stuff, renew items \r\n\r\nWe'll also show you some (conceptual) ideas about using the device camera for locating and checking out materials.\r\n\r\nOn a general level, what we are trying to achieve is a move away from a web based paradigm and establish new ways of interaction better suited to the new devices and on their own terms. The demonstration will feature working mobile prototypes including both native apps (iPhone) and web apps. In both cases they will run on live data from our OPAC on www.statsbiblioteket.dk/search/\r\n\r\nThis talk is actually also a continuation of our Code4Lib 2010 talk called [\"Kill The Search Button\"](http://code4lib.org/conference/2010/schedule), which we unfortunately never got around to do, due to a Danish blizzard.","score":195},{"title":"Speaking in code: talking tech with humans (and librarians)","id":402,"description":"Erin White, Virginia Commonwealth University Libraries, erwhite@vcu.edu \r\n\r\nWe do awesome work, right? But what's the best way to communicate that work with non-geek stakeholders within our organizations? I'll present some ideas on how to communicate tech with those who don't always speak the language fluently. This'll include pitching new projects; communicating about existing projects; and dealing with project maintenance and problem-solving. I'll share some tips for explaining systems changes and problems, how to use help tickets as teachable moments for you or librarians, updating documentation, etc. ","score":131},{"title":"Building a Code4Lib 2012 Conference Mobile App with the Kuali Mobility Framework ","id":403,"description":"* Michelle Suranofsky, Lehigh University, michelle dot suranofsky at lehigh dot edu\r\n* Tod Olson, University of Chicago, tod at uchicago dot edu \r\n\r\nHot off the heals of the Kuali Days 2011 Conference, we thought it would be fun to take the newly released Kuali Mobility for Enterprise framework for a test drive by creating a Code4Lib Conference Mobile App.\r\n\r\n[Kuali Mobility for Enterprise (KME)](http://kuali.org/mobility) is an open source framework for developing and deploying applications to connect mobile devices to an institution's information resources. Applications may be deployed as mobile websites or as installable apps. The KME framework makes heavy use of HTML5, CSS, and Javascript, and builds on other open source projects like PhoneGap and JQuery Mobile.\r\n\r\nWe will discuss the mechanics of the Kuali Mobility framework along with the experience using it to create a mobile app. for the Code4Lib conference. ","score":130},{"title":"The ARCHIVEMATICA digital preservation system ","id":404,"description":"* Peter Van Garderen, Archivematica Project Manager, Artefactual Systems, peter at artefactual dot com\r\n* Courtney Mumma, Archivematica Community Manager, courtney at artefactual dot com \r\n\r\nThe open source (AGPL3) [Archivematica](http://archivematica.org/) digital preservation system uses a micro-services architecture to integrate a suite of Linux utilities into workflow pipelines. It is designed as a backend tool for archivists and librarians managing digital collections and digital preservation responsibilities. We use Google Gearman for job scheduling and load balancing as well as Django (python) for a web-based administration interface that monitors and controls the processing of files in the pipelines. The system creates standards-compliant (e.g. METS, PREMIS, Bagit) archival packages as well as a registry interface to monitor format policies. This system is designed to provide the technical component for ISO 14721 (OAIS) and ISO 16363 (TRAC) compliant Trusthworthy Digital Repositories. The recent 0.8 release is the last alpha. Over winter 2012 we are continuing with scalability testing and tuning, adding ElasticSearch indexing, SWORD deposit support, interfaces for Dspace, ContentDM, XTF; all for inclusion in the 0.9-beta release sometime in Spring 2012. The presentation will give a quick demo of Archivematica's features as well as discuss technical architecture, APIs, development roadmap, user base, community building, project management, etc. ","score":148},{"title":"Virtual Integrated Search - on-the-fly merging of relevancy ranked searches ","id":405,"description":"    Mads Villadsen, The State and University Library Denmark, mv@statsbiblioteket.dk \r\n\r\nWhat do you do when you have an integrated search system and the users want data at the article level? What we did was to try and get the data from the publishers - and when that failed we went with Summon for the article data while keeping our bibliographic records (and more) in our own system.\r\n\r\nSo how\u2019s that working out for us?\r\n\r\nWe didn\u2019t want to give up on our overall goal of having a single unified result set which meant we had to do something out of the ordinary.\r\n\r\nWe struck a deal with Serials Solutions that allowed us to apply our technical know-how and sprinkle fairy dust on our queries thereby achieving a proper relevancy ranked merging of results from our own index with the results from Summon. We gave a lightning talk about some of these ideas at last year's code4lib.\r\n\r\nWe have been running this \"Virtual Integrated Search\" in production since August and the end users haven't come at us with their pitch forks yet so we assume they are still able to find what they are looking for.\r\n\r\nJust to be sure we will be performing a usability test in November 2011 that will hopefully guide our future development.\r\n\r\nI will cover what goes into making fairy dust (\"how it works\", \"what doesn't work\") as well as some of the results from the usability test (\"does it actually work?\").\r\n\r\n[http://www.statsbiblioteket.dk/search/](http://www.statsbiblioteket.dk/search/)","score":146},{"title":"Kuali Rice and preparing for OLE ","id":406,"description":"* Tod Olson, University of Chicago, tod at uchicago dot edu\r\n* Michelle Suranofsky, Lehigh University, michelle dot suranofsky at lehigh dot edu \r\n\r\nKuali Rice provides some of the fundamental underlying services for Kuali OLE and other Kuali software, services such as workflows, a service bus, integration with campus identity management, and more. In preparation for OLE, some partner libraries are developing their own simple Rice-base applications to provide some useful automation now while gaining experience that will prepare us for running Rice as part of OLE. This talk will give a brief overview of Kuali Rice and then discuss the construction of a real-but-simple Rice application. ","score":91},{"title":"Argo and DOR Services: The developer and administrative interfaces to Stanford's Digital Object Registry ","id":407,"description":"Michael B. Klein, Library Infrastructure Engineer, Stanford University Libraries, mbklein at stanford dot edu \r\n\r\nArgo is the administrative interface for Stanford's Digital Object Registry (DOR), the central repository of information about digital assets owned or managed by Stanford University Libraries and Academic Information Resources (SULAIR). Built on Blacklight, with help from other pieces of the Hydra repository framework, Argo provides a top-down, source-independent, application-agnostic view of items working their way through various stages of registration, submission, description, digitization, accessioning, publication, shelving, and preservation.\r\n\r\nArgo's functionality is provided through three separate layers:\r\n\r\n* A traditional web application, which provides UI-based bulk and individual item registration, management, and reporting functions\r\n* A web service, which provides RESTful access to several of the same functions\r\n* A DOR services Ruby gem which opens most of this functionality to other Ruby code, from Rails applications to accessioning daemons to one-off scripts \r\n\r\nThis presentation will explore Argo's full stack, from the underlying DOR Services gem (encapsulating a number of other disparate library infrastructure functions) to its use by SULAIR developers, contractors, digitization lab staff, project managers, and SULAIR technical staff. ","score":139},{"title":"The Way to Bulid C4L Activities in Your Homeland - Based on the Experience of Code4Lib JAPAN. ","id":408,"description":"Makoto Okamoto, Chief Editor of Academic Resource Guide (ARG) and Executive Officer of Code4Lib JAPAN, arg.editor_at_gmail.com \r\n\r\nIn August 2010, We launched the \"Code4Lib JAPAN\", a kind of local activities of Code4Lib in JAPAN after preparation for 6 months. Since then, Code4Lib JAPAN did a great sucess and growth. Approximately, activities of Code4Lib JAPAN are divided into 4 parts like operation of orgnization and activities, offer training program, proposing some guidelines, dispatching a mission to Code4Lib Conference and selection of good practice.\r\n\r\nIn this presentation, some key facters of our sucess and growth will be explained by Executive Officer of Code4Lib JAPAN. Those key facters like getting money from outside grant, indutrial sponsers and personal supporters, operation of orgnization and activities on a self-supporting basis will be very helpful for those who are wishing to launch local activitiy in their homeland. We can offer variuus tiips to spread value and activities of Code4Lib in the world. ","score":85},{"title":"The Golden Road (To Unlimited Devotion): Building a Socially Constructed Archive of Grateful Dead Artifacts ","id":409,"description":"* Robin Chandler, University of California (Santa Cruz), chandler [at] ucsc [dot] edu\r\n* Susan Chesley Perry, University of California (Santa Cruz), chesley [at] ucsc [dot] edu\r\n* Kevin S. Clarke, University of California (Santa Cruz), ksclarke [at] ucsc [dot] edu \r\n\r\nThe Grateful Dead Archive at the University of California (Santa Cruz) is a collection of over 600 linear feet of material, including: business records, photographs, posters, fan envelopes, tickets, video, audio (oral histories, interviews and music) and 3-d objects such as stage props and band merchandise. In addition, with the release of the Grateful Dead Archive Online website in 2012, the Archive will start actively collecting artifacts from an enthusiastic community of Grateful Dead fans.\r\n\r\nThis talk will discuss the challenges of merging a traditional archive with a socially constructed one. We will also present the first round of development and explain how we're using tools like Omeka, ContentDM, UC3 Merritt, djatoka, Kaltura, Google Maps, and Solr to lay the foundation for a robust and engaging site. Future directions, like the integration/development of better curation tools and what we hope to learn from opening the archive to contributions from a large community of fans, will also be discussed. ","score":159},{"title":"Library News - A gathering place for library and tech news, and more ","id":410,"description":"Matt Phillips, Harvard Library Innovation Lab, mphillips@law.harvard.edu \r\n\r\n[Library News](http://news.librarycloud.org/) is gathering place for people to share and discuss news from the technology and library worlds. Think [Hacker News](http://news.ycombinator.com/), but for library dorks instead of startup dorks.\r\n\r\nLibrary News is more than a news and discussion site, it analyzes submitted links and shares its observations. One example of this sharing is the exposure of popular blogs: Library News tracks submitted blog entries and tallies them up, creating a list of most popular blogs in the community. This most popular list is exposed as an HTML document and as an [OPML](http://en.wikipedia.org/wiki/OPML) download (The OPML file can be loaded directly into an RSS reader and be used as an always up-to-date \"starter pack\" of popular blogs in the library and tech spaces).\r\n\r\n\r\nMy rough talk outline:\r\n\r\n* Demo Library News\r\n* Present how Library News goes beyond normal discussion sites (the tools that allow to explore community submitted links)\r\n* Discuss where Library News fits with the current library news ecosystem \r\n\r\n\r\nFind more information about Library News at the [Library News FAQ](http://news.librarycloud.org/faq) \r\n","score":78},{"title":"Mining Wikipedia for Book Articles ","id":412,"description":"Paul Deschner, Harvard Library Innovation Lab, deschner@law.harvard.edu \r\n\r\nSuppose you were developing a browsing tool for library materials and wanted to include Wikipedia articles and categories whenever available -- how would you do it? There is no API or other data service which one can use to get a comprehensive listing of every page in Wikipedia devoted to the discussion of a book.\r\n\r\nThis talk will focus on the tools, workflows and data sources we have used to approach this problem. Tools and workflows include the use of Infobox ISBN's and other standard identifiers, analysis of Wikipedia categories and category hierarchies, exploitation of article abstracts and titles, and Mechanical Turk resources. Data sources include Dbpedia triple stores and Wikimedia XML/SQL dumps. So far, we have harvested around 60,000 book articles. This is an exploration in dealing with open, relatively unstructured Web content, and in aggregating answers to the same question using quite diverse techniques. ","score":154}]